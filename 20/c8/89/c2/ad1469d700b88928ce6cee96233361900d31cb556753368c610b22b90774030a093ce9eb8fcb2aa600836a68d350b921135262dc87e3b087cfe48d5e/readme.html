<h1 id="google-robots.txt-parser-and-matcher-library">Google Robots.txt Parser and Matcher Library</h1>
<p>The repository contains Google's robots.txt parser and matcher as a C++ library (compliant to C++11).</p>
<h2 id="about-the-library">About the library</h2>
<p>The Robots Exclusion Protocol (REP) is a standard that enables website owners to control which URLs may be accessed by automated clients (i.e. crawlers) through a simple text file with a specific syntax. It's one of the basic building blocks of the internet as we know it and what allows search engines to operate.</p>
<p>Because the REP was only a de-facto standard for the past 25 years, different implementers implement parsing of robots.txt slightly differently, leading to confusion. This project aims to fix that by releasing the parser that Google uses.</p>
<p>The library is slightly modified (i.e. some internal headers and equivalent symbols) production code used by Googlebot, Google's crawler, to determine which URLs it may access based on rules provided by webmasters in robots.txt files. The library is released open-source to help developers build tools that better reflect Google's robots.txt parsing and matching.</p>
<p>For webmasters, we included a small binary in the project that allows testing a single URL and user-agent against a robots.txt.</p>
<h2 id="building-the-library">Building the library</h2>
<p><a href="https://bazel.build/">Bazel</a> is the official build system for the library, which is supported on most major platforms (Linux, Windows, MacOS, for example) and compilers.</p>
<p>CMake support may be added in a future release. If you want to help us with adding CMake support, pull requests are highly welcome!</p>
<h2 id="quickstart">Quickstart</h2>
<p>We included with the library a small binary to test a local robots.txt against a user-agent and URL. Running the included binary requires:</p>
<ul>
<li>A compatible platform (e.g. Windows, Mac OS X, Linux, etc.). Most platforms are fully supported.</li>
<li>A compatible C++ compiler supporting at least C++11. Most major compilers are supported.</li>
<li><a href="https://git-scm.com/">Git</a> for interacting with the source code repository. To install Git, consult the <a href="https://help.github.com/articles/set-up-git/">Set Up Git</a> guide on <a href="http://github.com/">GitHub</a>.</li>
<li>Although you are free to use your own build system, most of the documentation within this guide will assume you are using <a href="https://bazel.build/">Bazel</a>. To download and install Bazel (and any of its dependencies), consult the <a href="https://docs.bazel.build/versions/master/install.html">Bazel Installation Guide</a>.</li>
</ul>
<p>To build and run the binary:</p>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash">$ <span class="fu">git</span> clone https://github.com/google/robotstxt.git robotstxt
<span class="ex">Cloning</span> into <span class="st">&#39;robotstxt&#39;</span>...
<span class="ex">...</span>
$ <span class="bu">cd</span> robotstxt/
<span class="ex">bazel-robots</span>$ bazel test :robots_test
<span class="ex">...</span>
/:<span class="ex">robots_test</span>                                                      PASSED in 0.1s

<span class="ex">Executed</span> 1 out of 1 test: 1 test passes.
<span class="ex">...</span>
<span class="ex">bazel-robots</span>$ bazel build :robots_main
<span class="ex">...</span>
<span class="ex">Target</span> //:robots_main up-to-date:
  <span class="ex">bazel-bin/robots_main</span>
<span class="ex">...</span>
<span class="ex">bazel-robots</span>$ bazel run robots_main -- ~/local/path/to/robots.txt YourBot http://example.com/url
  <span class="ex">user-agent</span> <span class="st">&#39;YourBot&#39;</span> with url <span class="st">&#39;http://example.com/url&#39;</span> allowed: YES</code></pre></div>
<h2 id="notes">Notes</h2>
<p>Parsing of robots.txt files themselves is done exactly as in the production version of Googlebot, including how percent codes and unicode characters in patterns are handled. The user must ensure however that the URI passed to the AllowedByRobots and OneAgentAllowedByRobots functions, or to the URI parameter of the robots tool, follows the format specified by RFC3986, since this library will not perform full normalization of those URI parameters. Only if the URI is in this format, the matching will be done according to the REP specification.</p>
<h2 id="license">License</h2>
<p>The robots.txt parser and matcher C++ library is licensed under the terms of the Apache license. See LICENSE for more information.</p>
<h2 id="links">Links</h2>
<p>To learn more about this project:</p>
<ul>
<li>check out the <a href="https://tools.ietf.org/html/draft-rep-wg-topic">internet draft</a>,</li>
<li>how <a href="https://developers.google.com/search/reference/robots_txt">Google's handling robots.txt</a>,</li>
<li>or for a high level overview, the <a href="https://en.wikipedia.org/wiki/Robots_exclusion_standard">robots.txt page on Wikipedia</a>.</li>
</ul>
