<p><a href="https://circleci.com/gh/STEllAR-GROUP/hpx/tree/master"><img src="https://circleci.com/gh/STEllAR-GROUP/hpx/tree/master.svg?style=svg" alt="HPX master branch build status" /></a> <a href="https://doi.org/10.5281/zenodo.598202"><img src="https://zenodo.org/badge/DOI/10.5281/zenodo.598202.svg" alt="zenodo_doi" /></a></p>
<p>Documentation: <a href="https://stellar-group.github.io/hpx/docs/sphinx/latest/html/index.html">latest</a>, <a href="https://stellar-group.github.io/hpx/docs/sphinx/branches/master/html/index.html">development (master)</a></p>
<h1 id="hpx">HPX</h1>
<p>HPX is a C++ Standard Library for Concurrency and Parallelism. It implements all of the corresponding facilities as defined by the C++ Standard. Additionally, in HPX we implement functionalities proposed as part of the ongoing C++ standardization process. We also extend the C++ Standard APIs to the distributed case.</p>
<p>The goal of HPX is to create a high quality, freely available, open source implementation of a new programming model for conventional systems, such as classic Linux based Beowulf clusters or multi-socket highly parallel SMP nodes. At the same time, we want to have a very modular and well designed runtime system architecture which would allow us to port our implementation onto new computer system architectures. We want to use real-world applications to drive the development of the runtime system, coining out required functionalities and converging onto a stable API which will provide a smooth migration path for developers.</p>
<p>The API exposed by HPX is not only modeled after the interfaces defined by the C++11/14/17/20 ISO standard, it also adheres to the programming guidelines used by the Boost collection of C++ libraries. We aim to improve the scalability of today's applications and to expose new levels of parallelism which are necessary to take advantage of the exascale systems of the future.</p>
<h2 id="whats-so-special-about-hpx">What's so special about HPX?</h2>
<ul>
<li>HPX exposes a uniform, standards-oriented API for ease of programming parallel and distributed applications.</li>
<li>It enables programmers to write fully asynchronous code using hundreds of millions of threads.</li>
<li>HPX provides unified syntax and semantics for local and remote operations.</li>
<li>HPX makes concurrency manageable with dataflow and future based synchronization.</li>
<li>It implements a rich set of runtime services supporting a broad range of use cases.</li>
<li>HPX exposes a uniform, flexible, and extendable performance counter framework which can enable runtime adaptivity</li>
<li>It is designed to solve problems conventionally considered to be scaling-impaired.</li>
<li>HPX has been designed and developed for systems of any scale, from hand-held devices to very large scale systems.</li>
<li>It is the first fully functional implementation of the ParalleX execution model.</li>
<li>HPX is published under a liberal open-source license and has an open, active, and thriving developer community.</li>
</ul>
<h2 id="documentation">Documentation</h2>
<p>If you plan to use HPX we suggest to start with the latest released version which can be downloaded <a href="https://stellar.cct.lsu.edu/downloads/">here</a>.</p>
<p>To quickly get started with HPX on most Linux distributions you can read the quick start guide <a href="https://stellar-group.github.io/hpx/docs/sphinx/latest/html/quickstart.html">here</a>. Detailed instructions on building and installing HPX on various platforms can be found <a href="https://stellar-group.github.io/hpx/docs/sphinx/latest/html/manual/building_hpx.html">here</a>. The full documentation for the latest release of HPX can always be found <a href="https://stellar-group.github.io/hpx/docs/sphinx/latest/html/index.html">here</a>.</p>
<p>If you would like to work with the cutting edge version of this repository (<code>master</code> branch) the documentation can be found <a href="https://stellar-group.github.io/hpx/docs/sphinx/branches/master/html/index.html">here</a>. We strongly recommend that you follow the current health status of the master branch by looking at our <a href="http://rostam.cct.lsu.edu/console">continuous integration results website</a>. While we try to keep the master branch stable and usable, sometimes new bugs trick their way into the code base. The <a href="https://circleci.com/gh/STEllAR-GROUP/hpx">CircleCI</a> continuous integration service additionally tracks the current build status for the master branch: <a href="https://circleci.com/gh/STEllAR-GROUP/hpx/tree/master"><img src="https://circleci.com/gh/STEllAR-GROUP/hpx/tree/master.svg?style=svg" alt="HPX master branch build status" /></a>.</p>
<p>If you can't find what you are looking for in the documentation or you suspect you've found a bug in HPX we very much encourage and appreciate any issue reports through the <a href="https://github.com/STEllAR-GROUP/hpx/issues">issue tracker for this Github project</a>.</p>
<p>If you have any questions feel free to ask it over at <a href="https://stackoverflow.com">StackOverflow</a> and tag the question with <a href="https://stackoverflow.com/questions/tagged/hpx">hpx</a>.</p>
<p>For a full list of support options please see our <a href="https://github.com/STEllAR-GROUP/hpx/blob/master/.github/SUPPORT.md">Support page</a>.</p>
<h2 id="code-of-conduct">Code of conduct</h2>
<p>We have adopted a <a href="https://github.com/STEllAR-GROUP/hpx/blob/master/.github/CODE_OF_CONDUCT.md">code of conduct</a> for this project. Please refer to this document if you would like to know more about the expectations for members of our community, with regard to how they will behave toward each other.</p>
<h2 id="citing">Citing</h2>
<p>In publications the latest release of HPX can be cited as: <a href="https://doi.org/10.5281/zenodo.598202"><img src="https://zenodo.org/badge/DOI/10.5281/zenodo.598202.svg" alt="zenodo_doi" /></a>.</p>
<h2 id="acknowledgements">Acknowledgements</h2>
<p>We would like to acknowledge the NSF, DoE, DTIC, DARPA, the Center for Computation and Technology (CCT) at Louisiana State University, and the Department of Computer Science 3 - Computer Architecture at the University of Erlangen Nuremberg who fund and support our work.</p>
<p>We would also like to thank the following organizations for granting us allocations of their compute resources: LSU HPC, LONI, XSEDE, NERSC, CSCS/ETHZ, and the Gauss Center for Supercomputing.</p>
<p>HPX is currently funded by</p>
<ul>
<li><p>The National Science Foundation through awards 1240655 (STAR), 1339782 (STORM), and 1737785 (Phylanx).</p>
<p>Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.</p></li>
<li><p>The Department of Energy (DoE) through the awards DE-AC52-06NA25396 (FLeCSI) and DE-NA0003525 (Resilience).</p>
<p>Neither the United States Government nor any agency thereof, nor any of their employees makes any warranty, express or implied, or assumes any legal liability or responsibility for the accuracy, completeness, or usefulness of any information, apparatus, product, or process disclosed, or represents that its use would not infringe privately owned rights. Reference herein to any specific commercial product, process, or service by trade name, trademark, manufacturer, or otherwise does not necessarily constitute or imply its endorsement, recommendation, or favoring by the United States Government or any agency thereof. The views and opinions of authors expressed herein do not necessarily state or reflect those of the United States Government or any agency thereof.</p></li>
<li><p>The Defense Technical Information Center (DTIC) under contract FA8075-14-D-0002/0007</p>
<p>Neither the United States Government nor any agency thereof, nor any of their employees makes any warranty, express or implied, or assumes any legal liability or responsibility for the accuracy, completeness, or usefulness of any information, apparatus, product, or process disclosed, or represents that its use would not infringe privately owned rights.</p></li>
<li>The Bavarian Research Foundation (Bayerische Forschungsstiftung) through the grant AZ-987-11.</li>
<li>The European Commission's Horizon 2020 programme through the grant H2020-EU.1.2.2. 671603 (AllScale).</li>
</ul>
